{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9983f804-16db-450b-83d2-e33a650f6edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch\n",
    "! pip install numpy\n",
    "! pip install rdkit\n",
    "! pip install allennlp-light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2464b25d-59fa-4c3f-85f9-c85cedd05e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Tokenizer(object):\n",
    "    def __init__(self, data):\n",
    "        unique_char = list(set(''.join(data))) + ['<eos>'] + ['<sos>']\n",
    "        self.mapping = {'<pad>': 0}\n",
    "        for i, c in enumerate(unique_char, start=1):\n",
    "            self.mapping[c] = i\n",
    "        self.inv_mapping = {v: k for k, v in self.mapping.items()}\n",
    "        self.start_token = self.mapping['<sos>']\n",
    "        self.end_token = self.mapping['<eos>']\n",
    "        self.vocab_size = len(self.mapping.keys())\n",
    "        \n",
    "    def encode_smile(self, mol, add_eos=True):\n",
    "        out = [self.mapping[i] for i in mol]\n",
    "        if add_eos:\n",
    "            out = out + [self.end_token]\n",
    "        return torch.LongTensor(out)\n",
    "\n",
    "    def batch_tokenize(self, batch):\n",
    "        out = map(lambda x: self.encode_smile(x), batch)\n",
    "        return torch.nn.utils.rnn.pad_sequence(list(out), batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373fb6e2-7788-4ad4-9e0e-9d1dd1aa14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from rdkit import Chem, RDLogger\n",
    "from torch import nn\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from layers import Generator, Discriminator\n",
    "from tokenizer import Tokenizer\n",
    "\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "\n",
    "class TrainGAN(nn.Module):\n",
    "\n",
    "    def __init__(self, data, hidden_dim=128, lr=1e-3, device='cpu'):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            data (list[str]): [description]\n",
    "            hidden_dim (int, optional): [description]. Defaults to 128.\n",
    "            lr ([type], optional): learning rate. Defaults to 1e-3.\n",
    "            device (str, optional): 'cuda' or 'cpu'. Defaults to 'cpu'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.tokenizer = Tokenizer(data)\n",
    "        \n",
    "        self.generator = Generator(\n",
    "            latent_dim=hidden_dim,\n",
    "            vocab_size=self.tokenizer.vocab_size - 1,\n",
    "            start_token=self.tokenizer.start_token - 1,  # no need token\n",
    "            end_token=self.tokenizer.end_token - 1,\n",
    "        ).to(device)\n",
    "        \n",
    "        self.discriminator = Discriminator(\n",
    "            hidden_size=hidden_dim,\n",
    "            vocab_size=self.tokenizer.vocab_size,\n",
    "            start_token=self.tokenizer.start_token,\n",
    "            bidirectional=True\n",
    "        ).to(device)\n",
    "\n",
    "        self.generator_optim = torch.optim.Adam(\n",
    "            self.generator.parameters(), lr=lr)\n",
    "\n",
    "        self.discriminator_optim = torch.optim.Adam(\n",
    "            self.discriminator.parameters(), lr=lr)\n",
    "\n",
    "        self.b = 0.  # baseline reward\n",
    "\n",
    "    def sample_latent(self, batch_size):\n",
    "        \"\"\"Sample from latent space\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): number of samples\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: [batch_size, self.hidden_dim]\n",
    "        \"\"\"\n",
    "        return torch.randn(batch_size, self.hidden_dim).to(self.device)\n",
    "\n",
    "    def discriminator_loss(self, x, y):\n",
    "        \"\"\"Discriminator loss\n",
    "\n",
    "        Args:\n",
    "            x (torch.LongTensor): input sequence [batch_size, max_len]\n",
    "            y (torch.LongTensor): sequence label (zeros from generatoe, ones from real data)\n",
    "                                  [batch_size, max_len]\n",
    "\n",
    "        Returns:\n",
    "            loss value\n",
    "        \"\"\"\n",
    "\n",
    "        y_pred, mask = self.discriminator(x).values()\n",
    "\n",
    "        loss = F.binary_cross_entropy(\n",
    "            y_pred, y, reduction='none') * mask\n",
    "\n",
    "        loss = loss.sum() / mask.sum()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, x):\n",
    "        \"\"\"One training step\n",
    "\n",
    "        Args:\n",
    "            x (torch.LongTensor): sample form real distribution\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, len_real = x.size()\n",
    "\n",
    "        # create real and fake labels\n",
    "        x_real = x.to(self.device)\n",
    "        y_real = torch.ones(batch_size, len_real).to(self.device)\n",
    "\n",
    "        # sample latent var\n",
    "        z = self.sample_latent(batch_size)\n",
    "        generator_outputs = self.generator.forward(z, max_len=498)\n",
    "        x_gen, log_probs, entropies = generator_outputs.values()\n",
    "\n",
    "        # label for fake data\n",
    "        _, len_gen = x_gen.size()\n",
    "        y_gen = torch.zeros(batch_size, len_gen).to(self.device)\n",
    "\n",
    "        #####################\n",
    "        # Train Discriminator\n",
    "        #####################\n",
    "\n",
    "        self.discriminator_optim.zero_grad()\n",
    "\n",
    "        # disc fake loss\n",
    "        fake_loss = self.discriminator_loss(x_gen, y_gen)\n",
    "\n",
    "        # disc real loss\n",
    "        real_loss = self.discriminator_loss(x_real, y_real)\n",
    "\n",
    "        # combined loss\n",
    "        discr_loss = 0.5 * (real_loss + fake_loss)\n",
    "        discr_loss.backward()\n",
    "\n",
    "        # clip grad\n",
    "        clip_grad_value_(self.discriminator.parameters(), 0.1)\n",
    "\n",
    "        # update params\n",
    "        self.discriminator_optim.step()\n",
    "\n",
    "        # ###############\n",
    "        # Train Generator\n",
    "        # ###############\n",
    "\n",
    "        self.generator_optim.zero_grad()\n",
    "\n",
    "        # prediction for generated x\n",
    "        y_pred, y_pred_mask = self.discriminator(x_gen).values()\n",
    "\n",
    "        # Reward (see the ref paper)\n",
    "        R = (2 * y_pred - 1)\n",
    "\n",
    "        # reward len for each sequence\n",
    "        lengths = y_pred_mask.sum(1).long()\n",
    "\n",
    "        # list of rew of each sequences\n",
    "        list_rewards = [rw[:ln] for rw, ln in zip(R, lengths)]\n",
    "\n",
    "        # compute - (r - b) log x\n",
    "        generator_loss = []\n",
    "        for reward, log_p in zip(list_rewards, log_probs):\n",
    "\n",
    "            # substract the baseline\n",
    "            reward_baseline = reward - self.b\n",
    "\n",
    "            generator_loss.append((- reward_baseline * log_p).sum())\n",
    "\n",
    "        # mean loss + entropy reg\n",
    "        generator_loss = torch.stack(generator_loss).mean() - \\\n",
    "            sum(entropies) * 0.01 / batch_size\n",
    "\n",
    "        # baseline moving average\n",
    "        with torch.no_grad():\n",
    "            mean_reward = (R * y_pred_mask).sum() / y_pred_mask.sum()\n",
    "            self.b = 0.9 * self.b + (1 - 0.9) * mean_reward\n",
    "\n",
    "        generator_loss.backward()\n",
    "\n",
    "        clip_grad_value_(self.generator.parameters(), 0.1)\n",
    "\n",
    "        self.generator_optim.step()\n",
    "\n",
    "        return {'loss_disc': discr_loss.item(), 'mean_reward': mean_reward}\n",
    "\n",
    "    def create_dataloader(self, data, batch_size=128, shuffle=True, num_workers=5):\n",
    "        \"\"\"create a dataloader\n",
    "\n",
    "        Args:\n",
    "            data (list[str]): list of molecule smiles\n",
    "            batch_size (int, optional): Defaults to 128.\n",
    "            shuffle (bool, optional): Defaults to True.\n",
    "            num_workers (int, optional): Defaults to 5.\n",
    "\n",
    "        Returns:\n",
    "            torch.data.DataLoader: a torch dataloader\n",
    "        \"\"\"\n",
    "\n",
    "        return DataLoader(\n",
    "            data,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=self.tokenizer.batch_tokenize,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "\n",
    "    def train_n_epochs(self, train_loader, max_epoch=10, evaluate_every=1):\n",
    "        \"\"\"Train for max_epoch epochs\n",
    "    \n",
    "        Args:\n",
    "            train_loader (torch.data.DataLoader): dataloader\n",
    "            max_epoch (int, optional): Defaults to 10.\n",
    "            evaluate_every (int, optional): Defaults to 1.\n",
    "        \"\"\"\n",
    "    \n",
    "        for epoch in range(max_epoch):\n",
    "            print(f\"Epoch {epoch + 1}/{max_epoch}\")\n",
    "    \n",
    "            iter_loader = iter(train_loader)\n",
    "            \n",
    "            for step, batch in enumerate(iter_loader):\n",
    "                # model update\n",
    "                self.train_step(batch)\n",
    "    \n",
    "                if step % evaluate_every == 0:\n",
    "                    self.eval()\n",
    "                    score = self.evaluate_n(100)\n",
    "                    self.train()\n",
    "    \n",
    "                    print(f'Valid {step/100} = {score:.2f}')\n",
    "\n",
    "    def get_mapped(self, seq):\n",
    "        \"\"\"Transform a sequence of ids to string\n",
    "\n",
    "        Args:\n",
    "            seq (list[int]): sequence of ids\n",
    "\n",
    "        Returns:\n",
    "            str: string output\n",
    "        \"\"\"\n",
    "        return ''.join([self.tokenizer.inv_mapping[i] for i in seq])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_n(self, n):\n",
    "        \"\"\"Generate n molecules\n",
    "\n",
    "        Args:\n",
    "            n (int)\n",
    "\n",
    "        Returns:\n",
    "            list[str]: generated molecules\n",
    "        \"\"\"\n",
    "\n",
    "        z = torch.randn((n, self.hidden_dim)).to(self.device)\n",
    "\n",
    "        x = self.generator(z)['x'].cpu()\n",
    "\n",
    "        lenghts = (x > 0).sum(1)\n",
    "\n",
    "        # l - 1 because we exclude end tokens\n",
    "        return [self.get_mapped(x[:l-1].numpy()) for x, l in zip(x, lenghts)]\n",
    "\n",
    "    def evaluate_n(self, n):\n",
    "        \"\"\"Evaluation: frequence of valid molecules using rdkit\n",
    "\n",
    "        Args:\n",
    "            n (int): number of sample\n",
    "\n",
    "        Returns:\n",
    "            float: requence of valid molecules\n",
    "        \"\"\"\n",
    "\n",
    "        pack = self.generate_n(n)\n",
    "\n",
    "        print(pack[0])\n",
    "\n",
    "        valid = np.array([Chem.MolFromSmiles(k) is not None for k in pack])\n",
    "\n",
    "        return valid.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd433ea7-551d-4845-afc2-ba4c9afaaab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from allennlp_light.modules.feedforward import FeedForward\n",
    "from allennlp_light.modules.seq2seq_encoders import (LstmSeq2SeqEncoder,\n",
    "                                               PytorchTransformer)\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "from torch.nn.modules.activation import Sigmoid\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim, vocab_size, start_token, end_token):\n",
    "        \"\"\"Generator\n",
    "\n",
    "        Args:\n",
    "            latent_dim (int): [description]\n",
    "            vocab_size (int): vocab size without padding\n",
    "            start_token ([int]): start token (without padding idx)\n",
    "            end_token ([int]): end token (without padding idx)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # (-1) we do not need pad token for the generator\n",
    "        self.vocab_size = vocab_size\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(self.vocab_size, latent_dim)\n",
    "\n",
    "        self.project = FeedForward(\n",
    "            input_dim=latent_dim,\n",
    "            num_layers=2,\n",
    "            hidden_dims=[latent_dim * 2, latent_dim * 2],\n",
    "            activations=[nn.LeakyReLU(negative_slope=0.01), nn.ELU(alpha=0.1)],\n",
    "            dropout=[0.1, 0.1]\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.LSTMCell(latent_dim, latent_dim)\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim * 2),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(latent_dim * 2, vocab_size - 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z, max_len=1005):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            z (torch.Tensor): [description]\n",
    "            max_len (int, optional): [description]. Defaults to 1005.\n",
    "\n",
    "        Returns:\n",
    "            dict: x [B, max_len], log_probabilities [B, max_len, vocab], entropies [B,]\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = z.shape[0]\n",
    "\n",
    "        # start of sequence\n",
    "        starts = torch.full(\n",
    "            size=(batch_size,), fill_value=self.start_token, device=z.device).long()\n",
    "\n",
    "        # embed_start\n",
    "        emb = self.embedding_layer(starts)\n",
    "\n",
    "        x = []\n",
    "        log_probabilities = []\n",
    "        entropies = []\n",
    "\n",
    "        h, c = self.project(z).chunk(2, dim=1)\n",
    "\n",
    "        for i in range(max_len):\n",
    "            \n",
    "            # new state\n",
    "            h, c = self.rnn(emb, (h, c))\n",
    "\n",
    "            # prediction\n",
    "            logits = self.output_layer(h)\n",
    "\n",
    "            # create dist\n",
    "            dist = Categorical(logits=logits)\n",
    "\n",
    "            # sample\n",
    "            sample = dist.sample()\n",
    "\n",
    "            # append prediction\n",
    "            x.append(sample)\n",
    "\n",
    "            # append log prob\n",
    "            log_probabilities.append(dist.log_prob(sample))\n",
    "\n",
    "            # append entropy\n",
    "            entropies.append(dist.entropy())\n",
    "\n",
    "            # new embedding\n",
    "            emb = self.embedding_layer(sample)\n",
    "\n",
    "        # stack along sequence dim\n",
    "        x = torch.stack(x, dim=1)\n",
    "        log_probabilities = torch.stack(log_probabilities, dim=1)\n",
    "        entropies = torch.stack(entropies, dim=1)\n",
    "\n",
    "        # keep only valid lengths (before EOS)\n",
    "        end_pos = (x == self.end_token).float().argmax(dim=1).cpu()\n",
    "\n",
    "        # sequence length is end token position + 1\n",
    "        seq_lengths = end_pos + 1\n",
    "\n",
    "        # if end_pos = 0 => put seq_length = max_len\n",
    "        seq_lengths.masked_fill_(seq_lengths == 1, max_len)\n",
    "\n",
    "        # select up to length\n",
    "        _x = []\n",
    "        _log_probabilities = []\n",
    "        _entropies = []\n",
    "        for x_i, logp, ent, length in zip(x, log_probabilities, entropies, seq_lengths):\n",
    "            _x.append(x_i[:length])\n",
    "            _log_probabilities.append(logp[:length])\n",
    "            _entropies.append(ent[:length].mean())\n",
    "\n",
    "        x = torch.nn.utils.rnn.pad_sequence(\n",
    "            _x, batch_first=True, padding_value=-1)\n",
    "\n",
    "        x = x + 1  # add padding token\n",
    "\n",
    "        return {'x': x, 'log_probabilities': _log_probabilities, 'entropies': _entropies}\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, vocab_size, start_token, bidirectional=True):\n",
    "        \"\"\"Discriminator\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): model hidden size\n",
    "            vocab_size (int): vocabulary size\n",
    "            bidirectional (bool, optional): [description]. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.start_token = start_token\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n",
    "\n",
    "        self.rnn = LstmSeq2SeqEncoder(\n",
    "            hidden_size, hidden_size, num_layers=1, bidirectional=bidirectional)\n",
    "\n",
    "        if bidirectional:\n",
    "            hidden_size = hidden_size * 2\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 2),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size * 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            x ([type]): [description]\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, _ = x.size()\n",
    "\n",
    "        # append start token to the input\n",
    "        starts = torch.full(\n",
    "            size=(batch_size, 1), fill_value=self.start_token, device=x.device).long()\n",
    "\n",
    "        x = torch.cat([starts, x], dim=1)\n",
    "\n",
    "        mask = x > 0\n",
    "\n",
    "        # embed input [batch_size, max_len, hidden_size]\n",
    "        emb = self.embedding(x)\n",
    "\n",
    "        # contextualize representation\n",
    "        x = self.rnn(emb, mask)\n",
    "\n",
    "        # prediction for each sequence\n",
    "        out = self.fc(x).squeeze(-1)  # [B, max_len]\n",
    "\n",
    "        return {'out': out[:, 1:], 'mask': mask.float()[:, 1:]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca0cd2-8728-415b-bff2-b8c113b98187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "# load data\n",
    "data = []\n",
    "with open('filtered_smiles_dataset.csv', \"r\") as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        smile = line.strip()\n",
    "        data.append(smile)\n",
    "\n",
    "# create model\n",
    "gan_model = TrainGAN(data, hidden_dim=64, lr=1e-3, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e4b1f4-b790-4dbd-8b86-2760b0676f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "loader = gan_model.create_dataloader(data, batch_size=128, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f792960-d300-437f-9f45-fc0dae030396",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_model.train_n_epochs(loader, max_epoch=2, evaluate_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a0d7db-fa78-4213-8d9d-db3a1d88e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cc6f6c-2d27-4f8e-8cb8-e2a92280b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_list = gan_model.generate_n(8)\n",
    "smiles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4457be8-1e99-4bf8-a665-2ee2875cb847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Save generator and discriminator models\n",
    "# torch.save(gan_mol.generator.state_dict(), f\"generator.pth\")\n",
    "# torch.save(gan_mol.discriminator.state_dict(), f\"discriminator.pth\")\n",
    "\n",
    "# # Save tokenizer state\n",
    "# tokenizer_state = {\n",
    "#     'mapping': gan_mol.tokenizer.mapping,\n",
    "#     'inv_mapping': gan_mol.tokenizer.inv_mapping,\n",
    "#     'start_token': gan_mol.tokenizer.start_token,\n",
    "#     'end_token': gan_mol.tokenizer.end_token,\n",
    "#     'vocab_size': gan_mol.tokenizer.vocab_size\n",
    "# }\n",
    "# torch.save(tokenizer_state, f\"tokenizer.pth\")\n",
    "\n",
    "# # Save generator optimizer state\n",
    "# torch.save(gan_mol.generator_optim.state_dict(), f\"generator_optimizer.pth\")\n",
    "\n",
    "# # Save discriminator optimizer state\n",
    "# torch.save(gan_mol.discriminator_optim.state_dict(), f\"discriminator_optimizer.pth\")\n",
    "\n",
    "# print(\"Model saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
