{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4986a15c-bc9d-4813-974d-64e93c1476b1",
   "metadata": {},
   "source": [
    "# LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f453196-a351-4380-8691-5871e69b03a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model import TrainGAN\n",
    "from tokenizer import Tokenizer\n",
    "from layers import Generator, Discriminator\n",
    "\n",
    "# Load data\n",
    "data = []\n",
    "with open('NTD_filtered_smiles_dataset.csv', \"r\") as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        smile = line.strip()\n",
    "        data.append(smile)\n",
    "\n",
    "step = 340000\n",
    "\n",
    "# Define a function to read the top hyperparameters from the file\n",
    "def read_top_hyperparameters(file_path):\n",
    "    top_params = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            score, params = line.split(', Parameters: ')\n",
    "            params = eval(params.strip())\n",
    "            top_params.append(params)\n",
    "    return top_params\n",
    "\n",
    "# Read the top hyperparameters\n",
    "top_hyperparameters = read_top_hyperparameters(\"best_hyperparams.txt\")\n",
    "\n",
    "best_params_best = top_hyperparameters[0]\n",
    "hidden_dim_best = best_params_best['hidden_dim']\n",
    "lr_best = best_params_best['lr']\n",
    "dropout_best = best_params_best['dropout']\n",
    "batch_size_best = best_params_best['batch_size'] \n",
    "\n",
    "best_params_best\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "tokenizer_state = torch.load(f'models(test)/checkpoint_step_{step}/tokenizer.pth')\n",
    "tokenizer = Tokenizer(data)\n",
    "tokenizer.mapping = tokenizer_state['mapping']\n",
    "tokenizer.inv_mapping = tokenizer_state['inv_mapping']\n",
    "tokenizer.start_token = tokenizer_state['start_token']\n",
    "tokenizer.end_token = tokenizer_state['end_token']\n",
    "tokenizer.vocab_size = tokenizer_state['vocab_size']\n",
    "\n",
    "generator = Generator(\n",
    "    latent_dim=hidden_dim_best,\n",
    "    vocab_size=tokenizer.vocab_size - 1,\n",
    "    start_token=tokenizer.start_token - 1,\n",
    "    end_token=tokenizer.end_token - 1,\n",
    ").to(device)\n",
    "generator.load_state_dict(torch.load(f'models(test)/checkpoint_step_{step}/generator.pth'))\n",
    "\n",
    "discriminator = Discriminator(\n",
    "    hidden_size=hidden_dim_best,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    start_token=tokenizer.start_token,\n",
    "    bidirectional=True\n",
    ").to(device)\n",
    "discriminator.load_state_dict(torch.load(f'models(test)/checkpoint_step_{step}/discriminator.pth'))\n",
    "\n",
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=lr_best)\n",
    "generator_optimizer.load_state_dict(torch.load(f'models(test)/checkpoint_step_{step}/generator_optimizer.pth'))\n",
    "\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr_best)\n",
    "discriminator_optimizer.load_state_dict(torch.load(f'models(test)/checkpoint_step_{step}/discriminator_optimizer.pth'))\n",
    "\n",
    "gan_model_loaded = TrainGAN(data, hidden_dim=hidden_dim_best, lr=lr_best, device=device)\n",
    "gan_model_loaded.tokenizer = tokenizer\n",
    "gan_model_loaded.generator = generator\n",
    "gan_model_loaded.discriminator = discriminator\n",
    "gan_model_loaded.generator_optim = generator_optimizer\n",
    "gan_model_loaded.discriminator_optim = discriminator_optimizer\n",
    "\n",
    "gan_model_loaded.generator.output_layer[1].p = dropout_best\n",
    "gan_model_loaded.generator.output_layer[4].p = dropout_best\n",
    "gan_model_loaded.discriminator.fc[1].p = dropout_best\n",
    "gan_model_loaded.discriminator.fc[4].p = dropout_best\n",
    "gan_model_loaded.generator.project._dropout[0].p = dropout_best\n",
    "gan_model_loaded.generator.project._dropout[1].p = dropout_best\n",
    "\n",
    "print(gan_model_loaded.generator.output_layer[1].p)\n",
    "print(gan_model_loaded.generator.output_layer[4].p)\n",
    "print(gan_model_loaded.discriminator.fc[1].p)\n",
    "print(gan_model_loaded.discriminator.fc[4].p)\n",
    "print(gan_model_loaded.generator.project._dropout[0].p)\n",
    "print(gan_model_loaded.generator.project._dropout[1].p) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a95ba829-6ca5-4194-ad9a-ac075a1b5b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainGAN(\n",
       "  (generator): Generator(\n",
       "    (embedding_layer): Embedding(56, 256)\n",
       "    (project): FeedForward(\n",
       "      (_activations): ModuleList(\n",
       "        (0): LeakyReLU(negative_slope=0.01)\n",
       "        (1): ELU(alpha=0.1)\n",
       "      )\n",
       "      (_linear_layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (_dropout): ModuleList(\n",
       "        (0-1): 2 x Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (rnn): LSTMCell(256, 256)\n",
       "    (output_layer): Sequential(\n",
       "      (0): LeakyReLU(negative_slope=0.01)\n",
       "      (1): Dropout(p=0.3, inplace=False)\n",
       "      (2): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Dropout(p=0.3, inplace=False)\n",
       "      (5): Linear(in_features=512, out_features=55, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (discriminator): Discriminator(\n",
       "    (embedding): Embedding(57, 256, padding_idx=0)\n",
       "    (rnn): LstmSeq2SeqEncoder(\n",
       "      (_module): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): LeakyReLU(negative_slope=0.01)\n",
       "      (1): Dropout(p=0.3, inplace=False)\n",
       "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Dropout(p=0.3, inplace=False)\n",
       "      (5): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      (6): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan_model_loaded.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec43f51c-2491-46bc-98b9-6a816e738982",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_list = gan_model_loaded.generate_n(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ccb25c7-ce3d-4128-a758-6a41b02ae459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:  47273\n",
      "invalid:  2727\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "data = []\n",
    "valid = 0\n",
    "invalid = 0\n",
    "\n",
    "def check_validity(smile):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        if mol is not None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "for smile in smiles_list:\n",
    "    if check_validity(smile):\n",
    "        validity = 'valid'\n",
    "        valid += 1\n",
    "    else:\n",
    "        validity = 'invalid'\n",
    "        invalid += 1\n",
    "    data.append([smile, validity])\n",
    "\n",
    "print(\"valid: \", valid)\n",
    "print(\"invalid: \", invalid)\n",
    "df = pd.DataFrame(data, columns=['canonical_smiles', 'validity'])\n",
    "df = df.drop_duplicates(subset=['canonical_smiles'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "907b39ec-9351-4ed0-8d7f-fe9cb56c0a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>canonical_smiles</th>\n",
       "      <th>validity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COC(=O)c1ccc(OC(C)=O)cc1</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC(C)(C)Oc1ccc(/C=N/NC(=O)c2ccccc2)cc1</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O=C(NC1CCCCC1)N1CCN(Cc2ccccc2)C1</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O=C(NC1CCCCC1)c1ccccc1OCc1ccccc1</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O=C(NCCc1csc(-c2ccccc2)n1)c1ccccc1</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49983</th>\n",
       "      <td>CCCCCCCCCCCC/C(=N\\NC(=O)c1ccccc1)c1ccccc1Cl</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49989</th>\n",
       "      <td>O=C(O)C(=O)Oc1ccc(-c2ccccc2)cc1</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>COC(=O)c1ccc(-c2csc(-c3cccnc3)n2)cc1</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>CCCCOc1ccc([C@@H](COc2ccccc2)c(OC)cc1)c1ccccc1</td>\n",
       "      <td>invalid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>O=C(NC[C@H]1OC[C@H](O)[C@H]1O)[C@@H](NC(=O)c1c...</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13580 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        canonical_smiles validity\n",
       "0                               COC(=O)c1ccc(OC(C)=O)cc1    valid\n",
       "1                 CC(C)(C)Oc1ccc(/C=N/NC(=O)c2ccccc2)cc1    valid\n",
       "2                       O=C(NC1CCCCC1)N1CCN(Cc2ccccc2)C1    valid\n",
       "3                       O=C(NC1CCCCC1)c1ccccc1OCc1ccccc1    valid\n",
       "4                     O=C(NCCc1csc(-c2ccccc2)n1)c1ccccc1    valid\n",
       "...                                                  ...      ...\n",
       "49983        CCCCCCCCCCCC/C(=N\\NC(=O)c1ccccc1)c1ccccc1Cl    valid\n",
       "49989                    O=C(O)C(=O)Oc1ccc(-c2ccccc2)cc1    valid\n",
       "49995               COC(=O)c1ccc(-c2csc(-c3cccnc3)n2)cc1    valid\n",
       "49997     CCCCOc1ccc([C@@H](COc2ccccc2)c(OC)cc1)c1ccccc1  invalid\n",
       "49998  O=C(NC[C@H]1OC[C@H](O)[C@H]1O)[C@@H](NC(=O)c1c...    valid\n",
       "\n",
       "[13580 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f3223b4-825e-487b-b957-79abf168401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('generated_molecules1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cdaaf1-b708-48ef-97c3-d61cd8586185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
